# ğŸ¤ Voice Content Platform

Transform your voice into professional content across multiple platforms while preserving your original message exactly as spoken.

## âœ¨ Features

- **ğŸ¤ Multi-Language Voice Input**: Support for 12+ languages including Gujarati, English, Spanish, French, German, and more
- **ğŸŒ Language Translation**: Speak in one language, get content in another
- **â¸ï¸ Pause & Resume**: Take breaks while recording without losing your input
- **ğŸ¤– AI-Powered Generation**: Creates content for LinkedIn, Twitter, blog posts, and podcast scripts
- **ğŸ“± Responsive Design**: Works perfectly on all devices
- **ğŸ”’ Content Preservation**: Your original speech input is never altered or enhanced

## ğŸš€ Tech Stack

- **Frontend & Backend**: Next.js 14 (App Router)
- **Styling**: Tailwind CSS
- **Language**: TypeScript
- **AI**: OpenAI GPT-4
- **Speech Recognition**: Web Speech API
- **Deployment**: Ready for Vercel, Netlify, or any hosting platform

## ğŸ› ï¸ Installation

### Prerequisites
- Node.js 18+ 
- npm or yarn
- OpenAI API key

### Setup Steps

1. **Clone the repository**
   ```bash
   git clone <your-repo-url>
   cd voice-content-platform
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Environment Configuration**
   ```bash
   cp .env.example .env.local
   ```
   
   Edit `.env.local` and add your OpenAI API key:
   ```env
   OPENAI_API_KEY=your_actual_openai_api_key_here
   NEXT_PUBLIC_APP_NAME=Voice Content Platform
   ```

4. **Run the development server**
   ```bash
   npm run dev
   ```

5. **Open your browser**
   Navigate to [http://localhost:3000](http://localhost:3000)

## ğŸŒ Supported Languages

### Input Languages (Speech Recognition)
- **Gujarati** (àª—à«àªœàª°àª¾àª¤à«€) - Default
- **English** (English)
- **Hindi** (à¤¹à¤¿à¤¨à¥à¤¦à¥€)
- **Spanish** (EspaÃ±ol)
- **French** (FranÃ§ais)
- **German** (Deutsch)
- **Italian** (Italiano)
- **Portuguese** (PortuguÃªs)
- **Russian** (Ğ ÑƒÑÑĞºĞ¸Ğ¹)
- **Japanese** (æ—¥æœ¬èª)
- **Korean** (í•œêµ­ì–´)
- **Chinese** (ä¸­æ–‡)

### Output Languages (Content Generation)
All input languages can be selected as output languages, allowing for complete language flexibility.

## ğŸ“– How to Use

1. **Select Languages**: Choose your input (speech) and output (content) languages
2. **Start Recording**: Click "Start Recording" and speak naturally
3. **Pause if Needed**: Use pause/resume to take breaks while thinking
4. **Complete Input**: Click "Done" when finished with your speech
5. **Generate Content**: AI processes your input and creates platform-specific content
6. **Copy & Use**: Copy, download, or share your generated content

## ğŸ¯ Content Types Generated

- **LinkedIn Posts**: Professional, engaging content for business networking
- **Twitter Threads**: Concise, shareable content optimized for Twitter
- **Blog Articles**: Detailed, SEO-friendly blog post content
- **Podcast Scripts**: Structured scripts perfect for audio content creation

## ğŸ”§ Configuration

### OpenAI API Setup
1. Get your API key from [OpenAI Platform](https://platform.openai.com/)
2. Add it to `.env.local`
3. The system will automatically use it for content generation

### Custom Language Support
Add new languages by editing `src/lib/languages.ts`:
```typescript
{
  code: 'xx',
  name: 'Language Name',
  nativeName: 'Native Name',
  speechRecognitionCode: 'xx-XX'
}
```

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ app/                    # Next.js App Router
â”‚   â”œâ”€â”€ api/               # API routes
â”‚   â””â”€â”€ page.tsx           # Main page
â”œâ”€â”€ components/             # React components
â”‚   â”œâ”€â”€ VoiceRecorder.tsx  # Voice input component
â”‚   â”œâ”€â”€ ContentProcessor.tsx # AI processing component
â”‚   â””â”€â”€ ContentDisplay.tsx # Results display component
â”œâ”€â”€ lib/                    # Utility libraries
â”‚   â”œâ”€â”€ languages.ts       # Language definitions
â”‚   â”œâ”€â”€ openai.ts          # OpenAI API integration
â”‚   â””â”€â”€ speechRecognition.ts # Speech recognition wrapper
â””â”€â”€ types/                  # TypeScript type definitions
```

## ğŸš€ Deployment

### Vercel (Recommended)
1. Push to GitHub
2. Connect repository to Vercel
3. Add environment variables in Vercel dashboard
4. Deploy automatically

### Other Platforms
- **Netlify**: Use `npm run build` and deploy `out/` folder
- **Railway**: Connect GitHub repo and add environment variables
- **Self-hosted**: Use `npm run build` and serve the `out/` folder

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT-4 API
- Next.js team for the amazing framework
- Web Speech API for browser-native speech recognition
- Tailwind CSS for the beautiful styling system

## ğŸ“ Support

If you encounter any issues or have questions:
1. Check the [Issues](https://github.com/yourusername/voice-content-platform/issues) page
2. Create a new issue with detailed information
3. Include your browser, OS, and any error messages

---

**Made with â¤ï¸ using Next.js, TypeScript, and Tailwind CSS**
